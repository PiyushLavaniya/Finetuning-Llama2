{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOrwlJEGBUSY"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install xformers\n",
        "!pip install -q datasets\n",
        "!pip install -q trl\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install -q bitsandbytes==0.37.2\n",
        "!pip install -q -U accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpNAMu9vBXjl"
      },
      "source": [
        "## Import the following libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65phwrUEBZMU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import bitsandbytes as bnb\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, PeftConfig\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, pipeline\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4GpeG7nBbY9"
      },
      "source": [
        "# Load a model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrGKrmhNBfjV"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leTZ3ihyBiVt"
      },
      "outputs": [],
      "source": [
        "repo_id = \"meta-llama/Llama-2-7b-chat-hf\"   #You can modify to whatever model you want to use.\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    repo_id,\n",
        "    device_map='auto',\n",
        "    load_in_8bit=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gt9a_23BpD_"
      },
      "outputs": [],
      "source": [
        "print(base_model)     #use it to check what target module should be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev7o9wSFBwjd"
      },
      "outputs": [],
      "source": [
        "base_model.get_memory_footprint()   #Check the memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfnsMAtTCRFR"
      },
      "source": [
        "# Test the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPi_C-WXCSKw"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\"\n",
        "\n",
        "def user_prompt(human_prompt):\n",
        "    prompt_template=f\"### HUMAN:\\n{human_prompt}\\n\\n### RESPONSE:\\n\" # This has to change if your dataset isn't formatted as Alpaca\n",
        "    return prompt_template\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=150,\n",
        "    repetition_penalty=1.15,\n",
        "    top_p=0.95\n",
        "    )\n",
        "result = pipe(user_prompt(\"You are an expert youtuber. Give me some ideas for a youtube title for a video about fine tuning LLM\"))\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85HJ5gQUCWC7"
      },
      "source": [
        "## Prepare and preprocess the model for training\n",
        "### You must know the 'target modules' for the model to specify them.\n",
        "### You can find the 'target modules' in the summary of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkz_1eHYCW7e"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # you have to know the target modules, it varies from model to model\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(base_model, config) # Wrap the base model with get_peft_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QFktYvAChQ7"
      },
      "source": [
        "# Load a dataset from datasets library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN4kcQ-ICiKz"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"csv\", data_files = \"you_data_here.csv\")     #substitute with whatever file name you have\n",
        "                                                                    ##I used the Dataset for Youtube Titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUQ_cG5oCsmk"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii3FjHhfCtoy"
      },
      "outputs": [],
      "source": [
        "adam_bits = 8\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir = \"Trainer_output\",\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    run_name=f\"deb-v2-xl-{adam_bits}bitAdam\",\n",
        "    logging_steps = 20,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16=True,\n",
        "    max_grad_norm = 0.3,\n",
        "    max_steps = 300,\n",
        "    warmup_ratio = 0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type = \"constant\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyOKARWLCvr6"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset[\"train\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    args = training_arguments,\n",
        "    max_seq_length = 512,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZHxNbOzCyeE"
      },
      "source": [
        "# Save the adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ0xHD1tCzZn"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"Finetuned_adapter\")\n",
        "adapter_model = model\n",
        "\n",
        "print(\"Lora Adapter saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgRpbB5uC34-"
      },
      "source": [
        "# Merge the base model and the adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD3vQTcaC6Sf"
      },
      "outputs": [],
      "source": [
        "# Can't merge the 8 bit/4 bit model with Lora so reload it\n",
        "\n",
        "repo_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "use_ram_optimized_load=False\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    repo_id,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "base_model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsxf16axC9Sf"
      },
      "outputs": [],
      "source": [
        "base_model.get_memory_footprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGEHrdc-DAzo"
      },
      "outputs": [],
      "source": [
        "# Loading Lora adapter\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"/content/Finetuned_adapter\",   ##Path to the Finetuned Adapter\n",
        "    )\n",
        "merged_model = model.merge_and_unload()\n",
        "\n",
        "merged_model.save_pretrained(\"/content/Merged_model\")\n",
        "tokenizer.save_pretrained(\"/content/Merged_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bfiNRSADDCY"
      },
      "source": [
        "## Testing out Fine Tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfPuoMSrDD5-"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\"\n",
        "\n",
        "def user_prompt(human_prompt):\n",
        "    prompt_template=f\"### HUMAN:\\n{human_prompt}\\n\\n### RESPONSE:\\n\"\n",
        "    return prompt_template\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=merged_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=150,\n",
        "    repetition_penalty=1.15,\n",
        "    top_p=0.95\n",
        "    )\n",
        "result = pipe(user_prompt(\"You are an expert youtuber. Give me some ideas for a youtube title for a video\"))\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioNOQiIXsIJm"
      },
      "source": [
        "##Pushing to HuggingfaceHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zMZI3HlDPI0"
      },
      "outputs": [],
      "source": [
        "merged_model.push_to_hub(\"your_huggingface_id/fine_tuned_model_name\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
